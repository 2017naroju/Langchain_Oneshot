{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "611e1aeac13c48cab1fa3f945a800936": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64b15397ef134320b072bb6ca8a5a585",
              "IPY_MODEL_9cfa626762484cc182bede56c49d62b9",
              "IPY_MODEL_a510d89503c14c8e9671e24f667056dd"
            ],
            "layout": "IPY_MODEL_da556a371b3249808b9cef558ff9d3cf"
          }
        },
        "64b15397ef134320b072bb6ca8a5a585": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0aa2c70be03b4bec8ce745ef000c4578",
            "placeholder": "​",
            "style": "IPY_MODEL_0ba40dbfc22a40d4b987cc3c378b7bdc",
            "value": "100%"
          }
        },
        "9cfa626762484cc182bede56c49d62b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9e4d4420f3d430ca720ff45132625da",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f2fc186030e94b5c8007911f6fc47457",
            "value": 20
          }
        },
        "a510d89503c14c8e9671e24f667056dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b91105a13d1644509a527f198037c80b",
            "placeholder": "​",
            "style": "IPY_MODEL_7f35d90e68104eebb19cdceb4b7fbcc6",
            "value": " 20/20 [00:51&lt;00:00,  2.55s/it]"
          }
        },
        "da556a371b3249808b9cef558ff9d3cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0aa2c70be03b4bec8ce745ef000c4578": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ba40dbfc22a40d4b987cc3c378b7bdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9e4d4420f3d430ca720ff45132625da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2fc186030e94b5c8007911f6fc47457": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b91105a13d1644509a527f198037c80b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f35d90e68104eebb19cdceb4b7fbcc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "87A0hxYRL5J4"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet  langchain langchain-openai langchain_community datasets pyarrow==15.0.2 google-search-results scipy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "SERP_API_KEY = userdata.get('SERP_API_KEY')\n"
      ],
      "metadata": {
        "id": "B1O2mPHkMUj6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
        " os.environ['SERP_API_KEY'] = SERP_API_KEY\n",
        "#  os.getenv('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "e10RO7hVM51P"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Evaluator\n",
        "from langchain.evaluation import load_evaluator\n",
        "\n",
        "eval_chain = load_evaluator(\"pairwise_string\")"
      ],
      "metadata": {
        "id": "CfzjECaNMF0H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(eval_chain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "3srrn_5eMLCY",
        "outputId": "735a2367-31ff-4388-8882-b7d747100f6f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain.evaluation.comparison.eval_chain.PairwiseStringEvalChain"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain.evaluation.comparison.eval_chain.PairwiseStringEvalChain</b><br/>def warning_emitting_wrapper(*args: Any, **kwargs: Any) -&gt; Any</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/langchain/evaluation/comparison/eval_chain.py</a>A chain for comparing two outputs, such as the outputs\n",
              " of two models, prompts, or outputs of a single model on similar inputs.\n",
              "\n",
              "Attributes:\n",
              "    output_parser (BaseOutputParser): The output parser for the chain.\n",
              "\n",
              "Example:\n",
              "    &gt;&gt;&gt; from langchain_community.chat_models import ChatOpenAI\n",
              "    &gt;&gt;&gt; from langchain.evaluation.comparison import PairwiseStringEvalChain\n",
              "    &gt;&gt;&gt; llm = ChatOpenAI(temperature=0, model_name=&quot;gpt-4&quot;, model_kwargs={&quot;random_seed&quot;: 42})\n",
              "    &gt;&gt;&gt; chain = PairwiseStringEvalChain.from_llm(llm=llm)\n",
              "    &gt;&gt;&gt; result = chain.evaluate_string_pairs(\n",
              "    ...     input = &quot;What is the chemical formula for water?&quot;,\n",
              "    ...     prediction = &quot;H2O&quot;,\n",
              "    ...     prediction_b = (\n",
              "    ...        &quot;The chemical formula for water is H2O, which means&quot;\n",
              "    ...        &quot; there are two hydrogen atoms and one oxygen atom.&quot;\n",
              "    ...     reference = &quot;The chemical formula for water is H2O.&quot;,\n",
              "    ... )\n",
              "    &gt;&gt;&gt; print(result)\n",
              "    # {\n",
              "    #    &quot;value&quot;: &quot;B&quot;,\n",
              "    #    &quot;comment&quot;: &quot;Both responses accurately state&quot;\n",
              "    #       &quot; that the chemical formula for water is H2O.&quot;\n",
              "    #       &quot; However, Response B provides additional information&quot;\n",
              "    # .     &quot; by explaining what the formula means.\\n[[B]]&quot;\n",
              "    # }</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 154);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.evaluation.loading import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"langchain-howto-queries\")"
      ],
      "metadata": {
        "id": "9A0lBeNFNnfe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk2CDKkLNs5l",
        "outputId": "bf986222-5dcd-4d8b-f369-f003ab4e2916"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentType, Tool, initialize_agent\n",
        "from langchain_community.utilities import SerpAPIWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize the language model\n",
        "# You can add your own OpenAI API key by adding openai_api_key=\"<your_api_key>\"\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\",api_key=os.getenv('OPENAI_API_KEY'))\n",
        "\n",
        "# Initialize the SerpAPIWrapper for search functionality\n",
        "# Replace <your_api_key> in openai_api_key=\"<your_api_key>\" with your actual SerpAPI key.\n",
        "search = SerpAPIWrapper(serpapi_api_key=SERP_API_KEY)\n",
        "\n",
        "# Define a list of tools offered by the agent\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Search\",\n",
        "        func=search.run,\n",
        "        coroutine=search.arun,\n",
        "        description=\"Useful when you need to answer questions about current events. You should ask targeted questions.\",\n",
        "    ),\n",
        "]"
      ],
      "metadata": {
        "id": "Z12L27TCO4Ej"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "functions_agent = initialize_agent(\n",
        "    tools, llm, agent=AgentType.OPENAI_MULTI_FUNCTIONS, verbose=False\n",
        ")\n",
        "conversations_agent = initialize_agent(\n",
        "    tools, llm, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=False\n",
        ")"
      ],
      "metadata": {
        "id": "5qSEHLWgPHbx"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "results = []\n",
        "agents = [functions_agent, conversations_agent]\n",
        "concurrency_level = 6  # How many concurrent agents to run. May need to decrease if OpenAI is rate limiting.\n",
        "\n",
        "# We will only run the first 20 examples of this dataset to speed things up\n",
        "# This will lead to larger confidence intervals downstream.\n",
        "batch = []\n",
        "for example in tqdm(dataset[:20]):\n",
        "    batch.extend([agent.acall(example[\"inputs\"]) for agent in agents])\n",
        "    if len(batch) >= concurrency_level:\n",
        "        batch_results = await asyncio.gather(*batch, return_exceptions=True)\n",
        "        results.extend(list(zip(*[iter(batch_results)] * 2)))\n",
        "        batch = []\n",
        "if batch:\n",
        "    batch_results = await asyncio.gather(*batch, return_exceptions=True)\n",
        "    results.extend(list(zip(*[iter(batch_results)] * 2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "611e1aeac13c48cab1fa3f945a800936",
            "64b15397ef134320b072bb6ca8a5a585",
            "9cfa626762484cc182bede56c49d62b9",
            "a510d89503c14c8e9671e24f667056dd",
            "da556a371b3249808b9cef558ff9d3cf",
            "0aa2c70be03b4bec8ce745ef000c4578",
            "0ba40dbfc22a40d4b987cc3c378b7bdc",
            "c9e4d4420f3d430ca720ff45132625da",
            "f2fc186030e94b5c8007911f6fc47457",
            "b91105a13d1644509a527f198037c80b",
            "7f35d90e68104eebb19cdceb4b7fbcc6"
          ]
        },
        "id": "V2PEuQM6Rhqu",
        "outputId": "9e9bb3e7-5bfe-4f8c-c70c-f0ad0c0687b4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "611e1aeac13c48cab1fa3f945a800936"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpgKR-xNUj2U",
        "outputId": "a3d4f5cd-d9fc-4173-b3e9-d25748512973"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[({'input': 'Can I use LangChain to automatically rate limit or retry failed API calls?',\n",
              "   'output': 'LangChain is a framework designed to simplify the creation of applications using large language models. It provides AI developers with tools to connect language models with external data sources and is supported by an active community. However, it is not specifically designed for automatically rate limiting or retrying failed API calls. \\n\\nTo automatically rate limit or retry failed API calls, you may need to use other tools or libraries that are specifically designed for that purpose, such as Axios for JavaScript or requests library for Python. These tools provide features for handling rate limiting, retries, and other HTTP request-related functionalities.'},\n",
              "  ValueError('An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: Final Answer')),\n",
              " ({'input': 'How can I ensure the accuracy and reliability of the travel data with LangChain?',\n",
              "   'output': \"To ensure the accuracy and reliability of travel data with LangChain, you can utilize LangChain's capabilities for Retrieval Augmented Generation (RAG) to provide additional context from specific data sources as part of the input to Language Models (LLMs). This technique can help improve the accuracy and reliability of the travel data by incorporating relevant information during the generation process. Additionally, you can leverage LangChain's comparison evaluators to measure and compare outputs from different chains or LLMs, which can be valuable for A/B testing and ensuring the quality of the generated data.\"},\n",
              "  {'input': 'How can I ensure the accuracy and reliability of the travel data with LangChain?',\n",
              "   'output': 'LangChain ensures the accuracy and reliability of travel data through features such as modular architecture, pre-trained models, memory features for state persistence, optimized algorithms, and hardware acceleration. It also offers comparison evaluators for measuring outputs and a flexible framework for building context-aware applications.'}),\n",
              " ({'input': 'How can I track student progress with LangChain?',\n",
              "   'output': 'To track student progress with LangChain, you can use LangSmith to help track token usage in your LLM application. LangSmith is a platform that helps to debug, test, evaluate, and monitor chains and agents built on any LLM framework. It seamlessly integrates with LangChain to provide insights into student progress and learning metrics.'},\n",
              "  ValueError('An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: ')),\n",
              " ({'input': 'langchain how to handle different document formats?',\n",
              "   'output': 'Handling different document formats involves several strategies such as avoiding saving unnecessary documents, following a consistent method for naming files and folders, storing related documents together, choosing a document with the desired format as a reference, and pasting content into that document. Some common document file formats include PDF, DOC, DOCX, HTML, HTM, XLS, XLSX, PPT, PPTX, ODP, KEY, and more. Design patterns like using object-based state machines and generic interfaces with abstraction methods can help in handling multiple file formats and versions effectively.'},\n",
              "  {'input': 'langchain how to handle different document formats?',\n",
              "   'output': 'To handle different document formats, it is recommended to avoid saving unnecessary documents, follow a consistent method for naming files and folders, store related documents together, choose a document with the desired format as a reference, and consider design patterns for handling multiple file types. Common document formats include PDF, DOC, DOCX, HTML, HTM, XLS, XLSX, PPT, PPTX, ODP, KEY, and more.'}),\n",
              " ({'input': 'Can I chain API calls to different services in LangChain?',\n",
              "   'output': \"Yes, you can chain API calls to different services in LangChain. You can make multiple API calls sequentially by calling one API after another in a chain. This allows you to pass data between different services and perform complex operations by combining the results of multiple API calls. If you'd like, I can help you search for more information on how to chain API calls in LangChain. Just let me know!\"},\n",
              "  {'input': 'Can I chain API calls to different services in LangChain?',\n",
              "   'output': 'Yes, it is possible to chain API calls to different services in LangChain by defining a custom tool that leverages the APIChain class to interact with the desired APIs based on their Swagger documentation.'}),\n",
              " ({'input': 'How do I handle API errors in my langchain app?',\n",
              "   'output': 'When handling API errors in your Langchain app, you can follow these steps:\\n\\n1. LangChain will retry up to 6 times on an exponential backoff if the model provider returns an error from their API. This enables error recovery.\\n\\n2. Update your initial prompt to provide better and more informative answers to populate error handling.\\n\\n3. Build error handling into your chains to mitigate failure modes by installing necessary packages.\\n\\n4. Ensure that your function calling the OpenAI API handles errors effectively to prevent crashes.\\n\\n5. Implement error handling when invoking tools in a LangChain application for a smoother interaction with the language model.\\n\\n6. Isolate error handling for specific errors like GraphRecursionError to allow the rest of your application to continue running smoothly.\\n\\n7. Troubleshoot and effectively handle common errors when using ChatOpenAI from Langchain.\\n\\n8. Manage parser failures in LangChain with systematic error handling to ensure smooth application functionality even in challenging situations.'},\n",
              "  ValueError('An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: {\\n  \"action\": \"Search\",\\n  \"action_input\": \"best practices for handling API errors in applications\"\\n}')),\n",
              " ({'input': 'How do I handle different currency and tax calculations with LangChain?',\n",
              "   'output': \"I couldn't find specific information on how to handle different currency and tax calculations with LangChain. Would you like me to search for more general information on currency and tax calculations?\"},\n",
              "  {'input': 'How do I handle different currency and tax calculations with LangChain?',\n",
              "   'output': \"Specific information on how LangChain handles different currency and tax calculations may require direct exploration of LangChain's official documentation or contacting their support for assistance.\"}),\n",
              " ({'input': 'How do I extract specific data from the document using langchain tools?',\n",
              "   'output': 'To extract specific data from a document using Langchain tools, you can use tools like Parseur, Octoparse, Parsehub, Web Scraper, Mozenda, Rossum, Docsumo, and Docparser. These tools are designed to help you extract relevant information from various types of documents, whether digital or in print. You can also automate the extraction process to save time and effort.'},\n",
              "  ValueError('An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: Final Answer')),\n",
              " ({'input': 'Can I use LangChain to handle real-time data from these APIs?',\n",
              "   'output': \"LangChain is a framework designed to simplify the creation of applications using large language models. It provides tools to connect language models with external data sources and is supported by an active community. Real-time data, on the other hand, refers to information that is delivered immediately after collection without any delay. It is often used for navigation, tracking, and real-time analytics.\\n\\nTo handle real-time data from APIs using LangChain, you can leverage LangChain's capabilities to connect to external data sources and process real-time information using language models. LangChain can help in processing and analyzing real-time data streams efficiently.\"},\n",
              "  {'input': 'Can I use LangChain to handle real-time data from these APIs?',\n",
              "   'output': 'Yes, you can use LangChain to handle real-time data from APIs.'}),\n",
              " ({'input': 'Can I use LangChain to track and manage travel alerts and updates?',\n",
              "   'output': 'LangChain is a framework designed to simplify the creation of applications using large language models. It provides tools to connect language models with external data sources and offers a standard interface for sequences of calls involving language models or other utilities. While LangChain can be used to build context-aware reasoning applications, it may not specifically cater to tracking and managing travel alerts and updates out of the box. However, you can leverage its flexible abstractions and AI-first toolkit to potentially create a solution for tracking and managing travel alerts and updates by integrating relevant data sources and models.'},\n",
              "  ValueError('An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: I now know the final answer.')),\n",
              " ({'input': 'Can I use LangChain to create and grade quizzes from these APIs?',\n",
              "   'output': 'LangChain is a framework designed to simplify the creation of applications using large language models. It provides tools for AI developers to connect language models with external data sources. You can use LangChain to build context-aware, reasoning applications with flexible abstractions and an AI-first toolkit.\\n\\nYou can create quizzes using online quiz makers like Quizizz, which allow you to quickly and easily create interactive quizzes on various topics. Additionally, you can grade quizzes automatically or manually using tools like ProProfs Quiz Maker, which offer different grading methods such as regular grading, partial grading, and custom grading.\\n\\nWhen it comes to APIs, they are software interfaces that allow different computer programs to communicate with each other. APIs can simplify software development by enabling integration of data, services, and capabilities from other applications.'},\n",
              "  {'input': 'Can I use LangChain to create and grade quizzes from these APIs?',\n",
              "   'output': 'Yes, you can use LangChain to create and grade quizzes from these APIs.'}),\n",
              " ({'input': 'Can I use LangChain to automate data cleaning and preprocessing for the AI plugins?',\n",
              "   'output': 'LangChain is a framework designed to simplify the creation of applications using large language models. It provides tools to connect language models with external data sources and offers a standard interface for chaining interoperable components. While LangChain is not specifically focused on data cleaning and preprocessing, it can be used to automate various tasks related to AI applications, including data processing and analysis. You may explore how LangChain can be integrated into your workflow to automate data cleaning and preprocessing for AI plugins.'},\n",
              "  ValueError('An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: Final Answer')),\n",
              " ({'input': 'How can I ensure the accuracy and reliability of the financial data with LangChain?',\n",
              "   'output': 'To ensure accuracy and reliability of financial data with LangChain, it is essential to curate a diverse set of examples that cover a wide range of scenarios and edge cases. The more relevant and representative your examples are, the better your language model can learn to extract information accurately.'},\n",
              "  ValueError('An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: Final Answer')),\n",
              " ({'input': 'Can I integrate medical imaging tools with LangChain?',\n",
              "   'output': 'Integrating medical imaging tools with LangChain can be a powerful combination. LangChain can be used to develop more advanced medical imaging tools that can process and analyze large volumes of medical images, such as X-rays and ultrasounds. By integrating medical imaging tools with LangChain, you can create AI agents that can search, retrieve, analyze, and interpret medical images more effectively. This integration can provide radiologists with assistance in image interpretation and data extraction, ultimately improving the efficiency and accuracy of medical imaging analysis.'},\n",
              "  ValueError('An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: Final Answer')),\n",
              " ({'input': 'How do I ensure the privacy and security of the patient data with LangChain?',\n",
              "   'output': 'To ensure the privacy and security of patient data with LangChain, you can follow these steps:\\n\\n1. Use strong passwords and encryption: Use strong passwords and encryption to protect LangChain data.\\n2. Monitor LangChain applications for suspicious activity: Monitor LangChain applications for suspicious activity, such as unusual spikes in traffic or unexpected changes in behavior.'},\n",
              "  {'input': 'How do I ensure the privacy and security of the patient data with LangChain?',\n",
              "   'output': 'LangChain ensures the privacy and security of patient data through organizational, technical, and administrative measures designed to protect against unauthorized access.'}),\n",
              " ({'input': 'How do I handle authentication for APIs in LangChain?',\n",
              "   'output': 'To handle authentication for APIs in LangChain, you can follow these steps:\\n\\n1. Ensure that the API key (\"KEY\") is valid and has the necessary permissions.\\n2. Confirm that the API key is not expired.\\n3. LangSmith supports two types of API keys: Service Keys and Personal Access Tokens. Both types of tokens can be used to authenticate requests to the LangSmith.\\n4. You can set up a custom Authorization header for authentication with LangChain, either directly or through the Portkey AI Gateway.\\n\\nIf you need more specific guidance on how to authenticate and instantiate the OpenAI model in LangChain using a Client ID and Access Token, you can refer to the API documentation or seek further assistance.'},\n",
              "  ValueError('An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: Final Answer')),\n",
              " ({'input': 'Can I use LangChain to recommend personalized study materials?',\n",
              "   'output': \"LangChain is a framework designed to simplify the creation of applications using large language models. It provides tools to connect language models with external data sources and offers flexible abstractions for building context-aware, reasoning applications. LangChain allows developers to build applications powered by large language models by chaining interoperable components. It provides a standard interface for sequences of calls to language models or other utilities.\\n\\nTo recommend personalized study materials using LangChain, you can leverage its capabilities to analyze and summarize study materials, create chatbots for study assistance, and develop applications for personalized learning experiences. By utilizing LangChain's tools and abstractions, you can build context-aware study recommendation systems tailored to individual learning needs.\"},\n",
              "  ValueError('An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: Final Answer')),\n",
              " ({'input': 'How do I connect to the arXiv API using LangChain?',\n",
              "   'output': 'To connect to the arXiv API using LangChain, you can follow these steps:\\n\\n1. Install the arxiv python package using the following command:\\n```\\n%pip install --upgrade --quiet arxiv\\n```\\n\\n2. You can access the arXiv API to retrieve scholarly articles in various fields such as physics, mathematics, computer science, and more.\\n\\n3. Make sure to install the necessary packages like PyMuPDF and langchain-community integration for handling PDF files and integrating with LangChain.\\n\\n4. Utilize function calling to interact with the arXiv API and search the knowledge base for relevant information.\\n\\n5. You can build a semantic paper engine using RAG with LangChain, Chainlit copilot apps, and Literal AI observability for enhanced functionality.\\n\\n6. The arXiv API provides access to all arXiv data, search, and linking facilities through an easy-to-use programmatic interface.\\n\\n7. Retrieve scientific articles from Arxiv.org into the Document format for downstream processing.\\n\\n8. Build an article search app with LangChain and MyScale to perform semantic searches on arXiv database IDs.\\n\\n9. Refer to the API documentation for detailed information on using the API and its functionalities.\\n\\nThese steps will help you connect to the arXiv API using LangChain for accessing scholarly articles and research data.'},\n",
              "  {'input': 'How do I connect to the arXiv API using LangChain?',\n",
              "   'output': 'To connect to the arXiv API using LangChain, you need to install the arxiv python package and use it to conduct searches programmatically. The API provides access to a vast number of scholarly articles in various fields, allowing developers to interact with the arXiv data easily.'}),\n",
              " ({'input': 'How can I use LangChain to interact with educational APIs?',\n",
              "   'output': 'LangChain is a framework designed to simplify the creation of applications using large language models. It provides tools to connect language models with external data sources and is supported by an active community. LangChain allows developers to build context-aware, reasoning applications with flexible abstractions and an AI-first toolkit. You can deploy LangChain runnables and chains using LangServe. LangChain is open-source and provides a standard interface for sequences of calls involving language models or other utilities. It is essentially a library of abstractions for Python and JavaScript, representing common steps and concepts necessary to work with language models.'},\n",
              "  {'input': 'How can I use LangChain to interact with educational APIs?',\n",
              "   'output': 'To use LangChain to interact with educational APIs, you can refer to tutorials, official documentation, and practical examples available online.'}),\n",
              " ({'input': 'langchain how to sort retriever results - relevance or date?',\n",
              "   'output': 'When sorting Retriever results, you can use techniques like combining multiple result sets with different relevance indicators, modifying the relevance score of search results with boosting queries, and using structured queries to retrieve the top relevant documents. Additionally, you can specify a date format for sorting results based on date values.'},\n",
              "  {'input': 'langchain how to sort retriever results - relevance or date?',\n",
              "   'output': 'The method to sort retriever results in Langchain involves re-ordering documents after retrieval to position the most relevant documents at the extremes.'})]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for example, (res_a, res_b) in zip(dataset, results):\n",
        "  print(example[\"inputs\"])\n",
        "  print(res_a)\n",
        "  print(\"#\"*20)\n",
        "  print(res_b)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlJ0TEzaWQGl",
        "outputId": "f39e6a81-31ec-43d0-8581-c52ea0776936"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can I use LangChain to automatically rate limit or retry failed API calls?\n",
            "{'input': 'Can I use LangChain to automatically rate limit or retry failed API calls?', 'output': 'LangChain is a framework designed to simplify the creation of applications using large language models. It provides AI developers with tools to connect language models with external data sources and is supported by an active community. However, it is not specifically designed for automatically rate limiting or retrying failed API calls. \\n\\nTo automatically rate limit or retry failed API calls, you may need to use other tools or libraries that are specifically designed for that purpose, such as Axios for JavaScript or requests library for Python. These tools provide features for handling rate limiting, retries, and other HTTP request-related functionalities.'}\n",
            "####################\n",
            "An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: Final Answer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.random()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlJbzRAJZvgS",
        "outputId": "fde214f3-d503-43d9-c5b8-f56c77a4d0a0"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.879361988265433"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "\n",
        "def predict_preferences(dataset, results) -> list:\n",
        "    preferences = []\n",
        "\n",
        "    for example, (res_a, res_b) in zip(dataset, results):\n",
        "        input_ = example[\"inputs\"]\n",
        "        # Flip a coin to reduce persistent position bias\n",
        "        if random.random() < 0.5:\n",
        "            pred_a, pred_b = res_a, res_b\n",
        "            a, b = \"a\", \"b\"\n",
        "        else:\n",
        "            pred_a, pred_b = res_b, res_a\n",
        "            a, b = \"b\", \"a\"\n",
        "        eval_res = eval_chain.evaluate_string_pairs(\n",
        "            prediction=pred_a[\"output\"] if isinstance(pred_a, dict) else str(pred_a),\n",
        "            prediction_b=pred_b[\"output\"] if isinstance(pred_b, dict) else str(pred_b),\n",
        "            input=input_,\n",
        "        )\n",
        "        print(eval_res)\n",
        "        print(\"pred_a: \", pred_a)\n",
        "        print(\"*\"*20)\n",
        "        print(\"pred_b: \", pred_b)\n",
        "        if eval_res[\"value\"] == \"A\":\n",
        "            preferences.append(a)\n",
        "        elif eval_res[\"value\"] == \"B\":\n",
        "            preferences.append(b)\n",
        "        else:\n",
        "            preferences.append(None)  # No preference\n",
        "    return preferences"
      ],
      "metadata": {
        "id": "Fbmr4wXiRoMX"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preferences = predict_preferences(dataset[:1], results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whWpGmAkRyTw",
        "outputId": "d829e02a-2ef5-4045-c1d5-3cd2b3fbfeea"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'reasoning': \"Assistant A's response is not helpful or relevant to the user's question. It seems to be an error message rather than an answer. On the other hand, Assistant B provides a clear and accurate response, explaining what LangChain is and suggesting other tools that can be used for rate limiting or retrying failed API calls. Therefore, Assistant B's response is more helpful, relevant, accurate, and demonstrates a greater depth of thought. \\n\\nFinal verdict: [[B]]\", 'value': 'B', 'score': 0}\n",
            "pred_a:  An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: Final Answer\n",
            "********************\n",
            "pred_b:  {'input': 'Can I use LangChain to automatically rate limit or retry failed API calls?', 'output': 'LangChain is a framework designed to simplify the creation of applications using large language models. It provides AI developers with tools to connect language models with external data sources and is supported by an active community. However, it is not specifically designed for automatically rate limiting or retrying failed API calls. \\n\\nTo automatically rate limit or retry failed API calls, you may need to use other tools or libraries that are specifically designed for that purpose, such as Axios for JavaScript or requests library for Python. These tools provide features for handling rate limiting, retries, and other HTTP request-related functionalities.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preferences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH_k2bGdbsTH",
        "outputId": "c312468c-da11-4b0a-d16b-beabc05f761c"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preferences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g45sz9MEWyVY",
        "outputId": "473d9699-2a76-4831-dd2a-b0b2906ff0e0"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'a',\n",
              " 'a',\n",
              " 'a',\n",
              " 'b',\n",
              " 'a',\n",
              " 'b',\n",
              " 'a',\n",
              " 'a',\n",
              " 'a',\n",
              " 'b',\n",
              " 'a',\n",
              " 'a',\n",
              " 'a',\n",
              " 'a',\n",
              " 'a',\n",
              " 'a',\n",
              " 'a',\n",
              " 'b',\n",
              " 'a']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "name_map = {\n",
        "    \"a\": \"OpenAI Functions Agent\",\n",
        "    \"b\": \"Structured Chat Agent\",\n",
        "}\n",
        "counts = Counter(preferences)\n",
        "pref_ratios = {k: v / len(preferences) for k, v in counts.items()}\n",
        "for k, v in pref_ratios.items():\n",
        "    print(f\"{name_map.get(k)}: {v:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jD8QiQ3_R0sB",
        "outputId": "959abd9f-54ca-4564-aadf-de6801b529fd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI Functions Agent: 80.00%\n",
            "Structured Chat Agent: 20.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(preferences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWxYbGMTR-VW",
        "outputId": "6dc07cdc-2739-4377-dae7-cacd6b71aa38"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'a': 16, 'b': 4})"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "\n",
        "\n",
        "def wilson_score_interval(\n",
        "    preferences: list, which: str = \"a\", z: float = 1.96\n",
        ") -> tuple:\n",
        "    \"\"\"Estimate the confidence interval using the Wilson score.\n",
        "\n",
        "    See: https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval\n",
        "    for more details, including when to use it and when it should not be used.\n",
        "    \"\"\"\n",
        "    total_preferences = preferences.count(\"a\") + preferences.count(\"b\")\n",
        "    n_s = preferences.count(which)\n",
        "\n",
        "    if total_preferences == 0:\n",
        "        return (0, 0)\n",
        "\n",
        "    p_hat = n_s / total_preferences\n",
        "\n",
        "    denominator = 1 + (z**2) / total_preferences\n",
        "    adjustment = (z / denominator) * sqrt(\n",
        "        p_hat * (1 - p_hat) / total_preferences\n",
        "        + (z**2) / (4 * total_preferences * total_preferences)\n",
        "    )\n",
        "    center = (p_hat + (z**2) / (2 * total_preferences)) / denominator\n",
        "    lower_bound = min(max(center - adjustment, 0.0), 1.0)\n",
        "    upper_bound = min(max(center + adjustment, 0.0), 1.0)\n",
        "\n",
        "    return (lower_bound, upper_bound)"
      ],
      "metadata": {
        "id": "PVxzpTKvSRSi"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for which_, name in name_map.items():\n",
        "    low, high = wilson_score_interval(preferences, which=which_)\n",
        "    print(\n",
        "        f'The \"{name}\" would be preferred between {low:.2%} and {high:.2%} percent of the time (with 95% confidence).'\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5cZiDsvSaag",
        "outputId": "e82bc97e-8bac-40aa-ca78-95caeaacb848"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The \"OpenAI Functions Agent\" would be preferred between 58.40% and 91.93% percent of the time (with 95% confidence).\n",
            "The \"Structured Chat Agent\" would be preferred between 8.07% and 41.60% percent of the time (with 95% confidence).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade scipy"
      ],
      "metadata": {
        "id": "bTDSLzL_Sxys"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "preferred_model = max(pref_ratios, key=pref_ratios.get)\n",
        "successes = preferences.count(preferred_model)\n",
        "n = len(preferences) - preferences.count(None)\n",
        "p_value = stats.binomtest(successes, n, p=0.5, alternative=\"two-sided\").pvalue\n",
        "print(\n",
        "    f\"\"\"The p-value is {p_value:.5f}. If the null hypothesis is true (i.e., if the selected eval chain actually has no preference between the models),\n",
        "then there is a {p_value:.5%} chance of observing the {name_map.get(preferred_model)} be preferred at least {successes}\n",
        "times out of {n} trials.\"\"\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnF82jz2Sd2X",
        "outputId": "e1ef4638-a7a9-4da5-8900-0026b860fa8d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The p-value is 0.01182. If the null hypothesis is true (i.e., if the selected eval chain actually has no preference between the models),\n",
            "then there is a 1.18179% chance of observing the OpenAI Functions Agent be preferred at least 16\n",
            "times out of 20 trials.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pref_ratios"
      ],
      "metadata": {
        "id": "UuXj-KBvTPRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Bb4JX1sSl_D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}